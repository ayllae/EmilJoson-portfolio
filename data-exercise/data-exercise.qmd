---
title: "Data Exercise"
---
In this exercise, I will be exploring text data using the Bible (King James Version). The _gutenbergr_ package provides access to public domain works from the [Project Gutenberg Collection](gutenberg.org). I'll access the bible from this package, and explore the data using the R package _tidytext_. We'll load a few more that will help us through this exercise.

```{r setup, warning=FALSE, message=FALSE} 
library(tidytext) #let's try tidytext for handling text data
library(ggplot2) #loading ggplot2 for making exploratory figures
library(gutenbergr) #access the bible from here
library(wordcloud2) #we'll see if we can make wordclouds
library(dplyr)
library(stringr)
```

Let's check out how to load the bible from the gutenberg collection.

```{r}
find <- gutenberg_works() |> 
    filter(title == "The King James Version of the Bible") #look for the book
find
```

The KJV bible has the gutenberg_id 10. With that, we can download the book.

```{r}
bible <- gutenberg_download(10)
```

Let's take a quick look as to what this object looks like.

```{r}
summary(bible)
str(bible)
bible
```

It looks like it's a big chunk of text, as expected.It seems to have two columns: gutenberg_id (which is just 10) and text. The text appears to be individual lines. If that's the case, we then have 99,553 lines of text.

The first thing we can do is transform the data into the *tidy text format* using the _tidytext_ package.

```{r}
bible_tidy <- bible |> 
    unnest_tokens(word, text)
```

This breaks down the object into individual words.


```{r}
bible_tidy
```

And if our new object is broken into individual words, it looks like there's 852,462 words in this bible. Now that we have this text format, we can quickly look at the most commonly used words in the KJV bible.


```{r}
word_count <- bible_tidy |>
    count(word, sort=TRUE)

bible_tidy |>
    count(word, sort=TRUE) |>
    filter(n > 8970) |> 
    mutate(word = reorder(word,n)) |>
    ggplot(aes(n,word)) +
    geom_col()
```

I can't say I'm too surprised but these are honestly pretty boring words. Fortunately, we can remove boring words (called stop words in the package) so that we can take a look at more interesting words.

```{r}
bible_interesting <- bible_tidy |>
    anti_join(get_stopwords())

word_count2 <- bible_interesting |>
    count(word, sort=TRUE)
word_count2
```


Now those seem more like bible words. Let's make a wordcloud of these as well using the _wordcloud2_ package. It's even interactive where you can hover your mouse cursor on the word to tell you how many instances they've been used in the KJV bible.

```{r}
wordcloud2(word_count2, color = "random-light", backgroundColor = "white")
```


Interestingly, we can also take a look at two-word combinations called bigrams. Let's see if those are any more interesting than the most common words list.


```{r}
bible_bigrams <- bible |>
    unnest_tokens(bigram, text, token = "ngrams", n = 2) |>
    filter(!is.na(bigram))

bible_bigrams

bible_bigrams |>
    count(bigram, sort = TRUE)

bible_bigrams |>
    count(bigram, sort=TRUE) |>
    filter(n > 1931) |> 
    mutate(bigram = reorder(bigram,n)) |>
    ggplot(aes(n,bigram)) +
    geom_col()
```

Unfortunately, these bigrams aren't necessarily any more exciting than their solitary counterparts. Let's see one last statistic before we proceed to deeper analysis. Let's do a simple letter count.


```{r}
bible_letters <- bible |> 
    unnest_tokens(letter, text, token = "characters")

letters <- bible_letters|>
    count(letter, sort=TRUE)

ggplot(letters, aes(reorder(letter,n), y = n))+
    geom_col()+
    coord_flip()+
    labs(x = "Letter", y = "Frequency")
```

Moving on, one cool thing about text analysis is something called *sentiment analysis (or opinion mining)*. There are databases of words with certain associations like how words feel. For example, we can categorize words in the bible based on if they're positive or negative.

```{r}
bible_bing <- bible_tidy |>
    inner_join(get_sentiments("bing")) |>
    count(word, sentiment, sort=TRUE) |>
    ungroup()

bible_bing
```

Now those are awesome bible words. Let's see the top positive and negative words on graphs.

```{r}
bible_bing %>%
  group_by(sentiment) %>%
  slice_max(n, n = 10) %>% 
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(n, word, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free_y") +
  labs(x = "Contribution to sentiment",
       y = NULL)
```

Overall, this was a pretty good exercise in learning about tools for text analysis. I haven't done, or thought about this, before - so I'm curious what kind of fields heavily rely on these types of analyses.